{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analytics - Knowledge Graph, BERT, spaCy, NLTK - Notebook 04\n",
    "\n",
    "This noteboook covers some cool language modeling and natural language processing tools and methods. \\\n",
    "References: \n",
    "https://www.kaggle.com/code/pavansanagapati/knowledge-graph-nlp-tutorial-bert-spacy-nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# !python -m nltk.downloader all \n",
    "\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Tokenization:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Text', 'Analytics', '-', 'Knowledge', 'Graph', ',', 'BERT', ',', 'spaCy', ',', 'NLTK', '-', 'Notebook', '04']\n"
     ]
    }
   ],
   "source": [
    "example_text = \"Text Analytics - Knowledge Graph, BERT, spaCy, NLTK - Notebook 04\"\n",
    "print(word_tokenize(example_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Stop Words:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'how', 'should', 'hers', \"don't\", 've', \"hadn't\", 'that', 'any', \"won't\", 'being', 'if', 'after', 'just', 'theirs', 'for', 'these', 'shouldn', 'up', 'once', 'but', 'too', 'their', 'such', \"mustn't\", 'him', 'did', 'to', 'mightn', 'will', 'those', 'so', 'hadn', 'most', 'between', 'needn', 'there', 'while', 'you', 'myself', 'having', 'nor', 'now', 'who', 'have', \"wouldn't\", 'll', 'wouldn', 'ours', 'himself', 'the', 'out', 'its', 'all', 'your', 'hasn', 'didn', 'before', 'when', 'has', 'doing', 'doesn', 'through', 'm', 'y', 'off', 'weren', 'do', 'against', 'ourselves', 'had', 'be', 'in', \"weren't\", \"you'll\", 'we', 'am', \"you're\", 'wasn', 'a', 'both', 'can', 'an', \"that'll\", 'why', \"aren't\", 'i', 'by', 'over', 'he', 'down', \"should've\", 'with', 'and', 's', \"hasn't\", 'isn', 'his', 'them', 'until', 'are', 'during', 'at', 'here', 'yourselves', 'ma', 'yourself', 'aren', 'were', \"shouldn't\", 'themselves', 'herself', 'she', 'then', 'shan', 'does', 'under', \"wasn't\", 'on', 'below', 'is', \"you've\", 'her', 'again', \"haven't\", 'very', 'whom', 'me', 'than', 'ain', 're', 'not', 'my', 'was', 'yours', 'same', 'into', \"it's\", 'because', 'some', 'as', \"needn't\", 'it', \"shan't\", 'where', 'other', \"she's\", 'each', 'from', 'this', 'of', \"mightn't\", 'which', 'mustn', 'about', 'no', 'what', 'won', 'been', 'couldn', 'they', 'more', 'o', 'haven', 'our', 'd', 'own', 'few', 'don', \"couldn't\", \"doesn't\", 'or', 'only', 'further', \"didn't\", \"you'd\", 't', 'itself', 'above', \"isn't\"}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ruchi', 'good', 'creature', 'planet', 'called', 'Earth', '.']\n"
     ]
    }
   ],
   "source": [
    "example_text = \"Ruchi is a good creature on this planet called Earth.\"\n",
    "\n",
    "words = word_tokenize(example_text)\n",
    "\n",
    "filtered_sentence = []\n",
    "for w in words:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "        \n",
    "print(filtered_sentence) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Stemming:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ruchi is have coffe , but whi is ruchi have coffe when she know it is not good for health ?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "example_text = \"Ruchi is having coffee, but why is ruchi having coffee when she knows it is not good for health?\"\n",
    "\n",
    "sentences = sent_tokenize(example_text)\n",
    "stemmer = PorterStemmer()\n",
    "new_sentence = []\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words = word_tokenize(sentences[i])\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    new_sentence.append(' '.join(words))\n",
    "    \n",
    "print(new_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Lemmatization:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ruchi is having coffee , but why is ruchi having coffee when she know it is not good for health ?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "sentences = sent_tokenize(example_text)\n",
    "lemmtizer = WordNetLemmatizer()\n",
    "\n",
    "new__lemmatize_sentence = []\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words = word_tokenize(sentences[i])\n",
    "    words = [lemmtizer.lemmatize(word) for word in words]\n",
    "    new__lemmatize_sentence.append(' '.join(words))\n",
    "    \n",
    "print(new__lemmatize_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>POS Tagging:</b>\\\n",
    "Let's use a new sentence tokenizer, called the PunktSentenceTokenizer. This tokenizer is capable of unsupervised machine learning, so we can actually train it on any body of text that we use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "# Now, let's create our training and testing data:\n",
    "train_txt=\"Crocodiles (subfamily Crocodylinae) or true crocodiles are large aquatic reptiles that live throughout the tropics in Africa, Asia, the Americas and Australia. Crocodylinae, all of whose members are considered true crocodiles, is classified as a biological subfamily. A broader sense of the term crocodile, Crocodylidae that includes Tomistoma, is not used in this article. The term crocodile here applies to only the species within the subfamily of Crocodylinae. The term is sometimes used even more loosely to include all extant members of the order Crocodilia, which includes the alligators and caimans (family Alligatoridae), the gharial and false gharial (family Gavialidae), and all other living and fossil Crocodylomorpha.\"\n",
    "sample_text =\"Crocodiles are large aquatic reptiles which are carnivorous.Allegators belong to this same reptile species\"\n",
    "\n",
    "# Next, we can train the Punkt tokenizer like:\n",
    "cust_tokenizer = PunktSentenceTokenizer(train_txt)\n",
    "\n",
    "# Then we can actually tokenize, using:\n",
    "tokenized = cust_tokenizer.tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech Tagging Output\n",
      "[('Crocodiles', 'NNS'), ('are', 'VBP'), ('large', 'JJ'), ('aquatic', 'JJ'), ('reptiles', 'NNS'), ('which', 'WDT'), ('are', 'VBP'), ('carnivorous.Allegators', 'NNS'), ('belong', 'RB'), ('to', 'TO'), ('this', 'DT'), ('same', 'JJ'), ('reptile', 'NN'), ('species', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Speech Tagging Output\")\n",
    "def process_text():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            print(tagged)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "process_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Chunking:</b>\\\n",
    "Now that we know the parts of speech, we can do what is called chunking, and group words into hopefully meaningful chunks. One of the main goals of chunking is to group into what are known as \"noun phrases.\" These are phrases of one or more words that contain a noun, maybe some descriptive words, maybe a verb, and maybe something like an adverb. The idea is to group nouns with the words that are in relation to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunked Output\n",
      "(S\n",
      "  Crocodiles/NNS\n",
      "  are/VBP\n",
      "  (Chunk large/JJ aquatic/JJ)\n",
      "  reptiles/NNS\n",
      "  which/WDT\n",
      "  are/VBP\n",
      "  carnivorous.Allegators/NNS\n",
      "  belong/RB\n",
      "  to/TO\n",
      "  this/DT\n",
      "  (Chunk same/JJ)\n",
      "  reptile/NN\n",
      "  species/NNS)\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "# Now, let's create our training and testing data:\n",
    "train_txt=\"Crocodiles (subfamily Crocodylinae) or true crocodiles are large aquatic reptiles that live throughout the tropics in Africa, Asia, the Americas and Australia. Crocodylinae, all of whose members are considered true crocodiles, is classified as a biological subfamily. A broader sense of the term crocodile, Crocodylidae that includes Tomistoma, is not used in this article. The term crocodile here applies to only the species within the subfamily of Crocodylinae. The term is sometimes used even more loosely to include all extant members of the order Crocodilia, which includes the alligators and caimans (family Alligatoridae), the gharial and false gharial (family Gavialidae), and all other living and fossil Crocodylomorpha.\"\n",
    "sample_text =\"Crocodiles are large aquatic reptiles which are carnivorous.Allegators belong to this same reptile species\"\n",
    "\n",
    "# Next, we can train the Punkt tokenizer like:\n",
    "cust_tokenizer = PunktSentenceTokenizer(train_txt)\n",
    "\n",
    "# Then we can actually tokenize, using:\n",
    "\n",
    "tokenized = cust_tokenizer.tokenize(sample_text)\n",
    "print(\"Chunked Output\")\n",
    "def process_text():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            chunkGram = r\"\"\"Chunk:{<NNS.?>*<JJ>+}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            #chunked.draw()\n",
    "            print(chunked)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "process_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Chinking:</b>\\\n",
    "After a lot of chunking, we have some words in your chunk you still do not want, but you have no idea how to get rid of them by chunking. You may find that chinking is your solution.\n",
    "\n",
    "Chinking is a lot like chunking, it is basically a way for you to remove a chunk from a chunk. The chunk that you remove from your chunk is your chink."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chinked Output\n",
      "(S\n",
      "  (Chunk Crocodiles/NNS)\n",
      "  are/VBP\n",
      "  (Chunk large/JJ aquatic/JJ reptiles/NNS which/WDT)\n",
      "  are/VBP\n",
      "  (Chunk carnivorous.Allegators/NNS belong/RB)\n",
      "  to/TO\n",
      "  this/DT\n",
      "  (Chunk same/JJ reptile/NN species/NNS))\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "# Now, let's create our training and testing data:\n",
    "train_txt=\"Crocodiles (subfamily Crocodylinae) or true crocodiles are large aquatic reptiles that live throughout the tropics in Africa, Asia, the Americas and Australia. Crocodylinae, all of whose members are considered true crocodiles, is classified as a biological subfamily. A broader sense of the term crocodile, Crocodylidae that includes Tomistoma, is not used in this article. The term crocodile here applies to only the species within the subfamily of Crocodylinae. The term is sometimes used even more loosely to include all extant members of the order Crocodilia, which includes the alligators and caimans (family Alligatoridae), the gharial and false gharial (family Gavialidae), and all other living and fossil Crocodylomorpha.\"\n",
    "sample_text =\"Crocodiles are large aquatic reptiles which are carnivorous.Allegators belong to this same reptile species\"\n",
    "\n",
    "# Next, we can train the Punkt tokenizer like:\n",
    "cust_tokenizer = PunktSentenceTokenizer(train_txt)\n",
    "\n",
    "# Then we can actually tokenize, using:\n",
    "\n",
    "tokenized = cust_tokenizer.tokenize(sample_text)\n",
    "\n",
    "print(\"Chinked Output\")\n",
    "def process_text():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            chunkGram = r\"\"\"Chunk: {<.*>+}\n",
    "                                    }<VB.?|IN|DT|TO>+{\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            #chunked.draw()\n",
    "            print(chunked)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "process_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Named Entity Recognition:</b>\\\n",
    "One of the most major forms of chunking in natural language processing is called \"Named Entity Recognition.\" The idea is to have the machine immediately be able to pull out \"entities\" like people, places, things, locations, monetary figures, and more.\n",
    "\n",
    "This can be a bit of a challenge, but NLTK is this built in for us. There are two major options with NLTK's named entity recognition: either recognize all named entities, or recognize named entities as their respective type, like people, places, locations, etc.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entity Output\n",
      "(S\n",
      "  Crocodiles/NNS\n",
      "  are/VBP\n",
      "  large/JJ\n",
      "  aquatic/JJ\n",
      "  reptiles/NNS\n",
      "  which/WDT\n",
      "  are/VBP\n",
      "  carnivorous.Allegators/NNS\n",
      "  belong/RB\n",
      "  to/TO\n",
      "  this/DT\n",
      "  same/JJ\n",
      "  reptile/NN\n",
      "  species/NNS)\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "# Now, let's create our training and testing data:\n",
    "train_txt=\"Crocodiles (subfamily Crocodylinae) or true crocodiles are large aquatic reptiles that live throughout the tropics in Africa, Asia, the Americas and Australia. Crocodylinae, all of whose members are considered true crocodiles, is classified as a biological subfamily. A broader sense of the term crocodile, Crocodylidae that includes Tomistoma, is not used in this article. The term crocodile here applies to only the species within the subfamily of Crocodylinae. The term is sometimes used even more loosely to include all extant members of the order Crocodilia, which includes the alligators and caimans (family Alligatoridae), the gharial and false gharial (family Gavialidae), and all other living and fossil Crocodylomorpha.\"\n",
    "sample_text =\"Crocodiles are large aquatic reptiles which are carnivorous.Allegators belong to this same reptile species\"\n",
    "\n",
    "# Next, we can train the Punkt tokenizer like:\n",
    "cust_tokenizer = PunktSentenceTokenizer(train_txt)\n",
    "\n",
    "# Then we can actually tokenize, using:\n",
    "\n",
    "tokenized = cust_tokenizer.tokenize(sample_text)\n",
    "\n",
    "print(\"Named Entity Output\")\n",
    "def process_text():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            namedEnt = nltk.ne_chunk(tagged,binary = True)\n",
    "            namedEnt.draw()\n",
    "            print(namedEnt)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "process_text()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The Corpora</b>\\\n",
    "The NLTK corpus is a massive dump of all kinds of natural language data sets that are definitely worth taking a look at.\n",
    "\n",
    "Almost all of the files in the NLTK corpus follow the same rules for accessing them by using the NLTK module, but nothing is magical about them. These files are plain text files for the most part, some are XML and some are other formats, but they are all accessible by manual, or via the module and Python. Let's talk about viewing them manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1:5 And God called the light Day, and the darkness he called Night.', 'And the evening and the morning were the first day.', '1:6 And God said, Let there be a firmament in the midst of the waters,\\nand let it divide the waters from the waters.', '1:7 And God made the firmament, and divided the waters which were\\nunder the firmament from the waters which were above the firmament:\\nand it was so.', '1:8 And God called the firmament Heaven.', 'And the evening and the\\nmorning were the second day.', '1:9 And God said, Let the waters under the heaven be gathered together\\nunto one place, and let the dry land appear: and it was so.', '1:10 And God called the dry land Earth; and the gathering together of\\nthe waters called he Seas: and God saw that it was good.', '1:11 And God said, Let the earth bring forth grass, the herb yielding\\nseed, and the fruit tree yielding fruit after his kind, whose seed is\\nin itself, upon the earth: and it was so.', '1:12 And the earth brought forth grass, and herb yielding seed after\\nhis kind, and the tree yielding fruit, whose seed was in itself, after\\nhis kind: and God saw that it was good.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import gutenberg\n",
    "sample = gutenberg.raw(\"bible-kjv.txt\")\n",
    "tok = sent_tokenize(sample)\n",
    "print(tok[5:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><i>End.</i></b>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
