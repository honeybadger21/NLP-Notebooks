{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analytics - Knowledge Graph, BERT, spaCy, NLTK - Notebook 02\n",
    "\n",
    "This noteboook covers some cool language modeling and natural language processing tools and methods.\n",
    "\n",
    "References: \\\n",
    "https://arxiv.org/abs/1810.04805 \\\n",
    "https://cloud.google.com/ai-platform/training/docs/algorithms/bert-start \\\n",
    "https://www.kaggle.com/code/pavansanagapati/knowledge-graph-nlp-tutorial-bert-spacy-nltk \\\n",
    "https://towardsml.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial/ \\\n",
    "https://www.analyticsvidhya.com/blog/2019/09/demystifying-bert-groundbreaking-nlp-framework/ \\\n",
    "https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03 \\\n",
    "https://towardsml.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial/ \n",
    "\n",
    "<b>Bidirectional Encoder Representations from Transformers | BERT </b>\n",
    "\n",
    "BERT is a method of pre-training language representations. Pre-training refers to how BERT is first trained on a large source of text, such as Wikipedia. You can then apply the training results to other Natural Language Processing (NLP) tasks, such as question answering and sentiment analysis. \n",
    "\n",
    "BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. \n",
    "\n",
    "BERT was trained by masking 15% of the tokens with the goal to guess them. An additional objective was to predict the next sentence.\n",
    "\n",
    "<i>Side Note: Transformers (Attention Is All You Need); (Pre-Trained) Contextualized Word Embeddings (ELMO)</i>\n",
    "\n",
    "<b>Architecture</b>:\\\n",
    "The original BERT model was developed and trained by Google using TensorFlow. BERT is released in two sizes BERTBASE and BERTLARGE. The BASE model is used to measure the performance of the architecture comparable to another architecture and the LARGE model produces state-of-the-art results that were reported in the research paper. One of the main reasons for the good performance of BERT on different NLP tasks was the use of <b><u><i>Semi-Supervised Learning</i></u></b>. This means the model is trained for a specific task that enables it to understand the patterns of the language. After training the model (BERT) has language processing capabilities that can be used to empower other models that we build and train using supervised learning.\n",
    "\n",
    "BERT is basically an Encoder stack of transformer architecture. A transformer architecture is an encoder-decoder network that uses self-attention on the encoder side and attention on the decoder side. \n",
    "\n",
    "BERTBASE has 12 layers in the Encoder stack while BERTLARGE has 24 layers in the Encoder stack. These are more than the Transformer architecture described in the original paper (6 encoder layers). BERT architectures (BASE and LARGE) also have larger feedforward-networks (768 and 1024 hidden units respectively), and more attention heads (12 and 16 respectively) than the Transformer architecture suggested in the original paper. It contains 512 hidden units and 8 attention heads. BERTBASE contains 110M parameters while BERTLARGE has 340M parameters.\n",
    "\n",
    "In summary: \\\n",
    "BERT-Base: 12 layer Encoder / Decoder, d = 768, 110M parameters \\\n",
    "BERT-Large: 24 layer Encoder / Decoder, d = 1024, 340M parameters, where d is the dimensionality of the final hidden vector output by BERT. Both of these have a Cased and an Uncased version (the Uncased version converts all words to lowercase).\n",
    "\n",
    "This model takes CLS token as input first, then it is followed by a sequence of words as input. Here CLS is a classification token. It then passes the input to the above layers. Each layer applies self-attention, passes the result through a feedforward network after then it hands off to the next encoder. The model outputs a vector of hidden size (768 for BERT BASE). If we want to output a classifier from this model we can take the output corresponding to CLS token.\n",
    "\n",
    "<b>Why need such models?</b>\\\n",
    "Researchers have developed various techniques for training general purpose language representation models using the enormous piles of unannotated text on the web (this is known as pre-training). These general purpose pre-trained models can then be fine-tuned on smaller task-specific datasets, e.g., when working with problems like question answering and sentiment analysis. This approach results in great accuracy improvements compared to training on the smaller task-specific datasets from scratch. \n",
    ">Easy training, less data, good results \n",
    "\n",
    "<b>Core Idea:</b>\n",
    "In the pre-BERT world, a language model would have looked at this text sequence during training from either left-to-right or combined left-to-right and right-to-left. This one-directional approach works well for generating sentences — we can predict the next word, append that to the sequence, then predict the next to next word until we have a complete sentence.\n",
    "\n",
    "Now enters BERT, a language model which is bidirectionally trained (this is also its key technical innovation). This means we can now have a deeper sense of language context and flow compared to the single-direction language models.\n",
    "\n",
    "Instead of predicting the next word in a sequence, BERT makes use of a novel technique called <b><u><i>Masked LM (MLM)</i></u></b>: it randomly masks words in the sentence and then it tries to predict them. Masking means that the model looks in both directions and it uses the full context of the sentence, both left and right surroundings, in order to predict the masked word. Unlike the previous language models, it takes both the previous and next tokens into account at the same time. The existing combined left-to-right and right-to-left LSTM based models were missing this “same-time part”. (It might be more accurate to say that BERT is non-directional though.)\n",
    "\n",
    "<b>How does it work</b>?\\\n",
    "BERT relies on a Transformer (the attention mechanism that learns contextual relationships between words in a text). A basic Transformer consists of an encoder to read the text input and a decoder to produce a prediction for the task. Since BERT’s goal is to generate a language representation model, it only needs the encoder part. The input to the encoder for BERT is a sequence of tokens, which are first converted into vectors and then processed in the neural network. But before processing can start, BERT needs the input to be massaged and decorated with some extra metadata:\n",
    "\n",
    "><u>Token embeddings</u>: A token is added to the input word tokens at the beginning of the first sentence and a token is inserted at the end of each sentence. \\\n",
    "<u>Segment embeddings</u>: A marker indicating Sentence A or Sentence B is added to each token. This allows the encoder to distinguish between sentences. \\\n",
    "<u>Positional embeddings</u>: A positional embedding is added to each token to indicate its position in the sentence.\n",
    "\n",
    "<b>BERT is pre-trained on 2 NLP Tasks</b>: \n",
    ">1. Masked Language Modeling\n",
    "2. Next Sentence Prediction\n",
    "\n",
    "<b>MLM</b>:\\\n",
    "BERT is designed as a deeply bidirectional model. The network effectively captures information from both the right and left context of a token from the first layer itself and all the way through to the last layer. Traditionally, we had language models either trained to predict the next word in a sentence (right-to-left context used in GPT) or language models that were trained on a left-to-right context. This made our models susceptible to errors due to loss in information.\n",
    "\n",
    "Let us take an example to understand it better: Let’s say we have a sentence – “I love to read data science blogs on Kaggle”. We want to train a bi-directional language model. Instead of trying to predict the next word in the sequence, we can build a model to predict a missing word from within the sequence itself. Let’s replace “Kaggle” with “[MASK]”. This is a token to denote that the token is missing. We’ll then train the model in such a way that it should be able to predict “Kaggle” as the missing token: “I love to read data science blogs on [MASK].” This is the crux of a Masked Language Model. The authors of BERT also include some caveats to further improve this technique: To prevent the model from focusing too much on a particular position or tokens that are masked, the researchers randomly masked 15% of the words.\n",
    "\n",
    "The masked words were not always replaced by the masked tokens [MASK] because the [MASK] token would never appear during fine-tuning. So, the researchers used the below technique:\n",
    "1. 80% of the time the words were replaced with the masked token [MASK]\n",
    "2. 10% of the time the words were replaced with random words\n",
    "3. 10% of the time the words were left unchanged\n",
    "\n",
    "<b>NSP</b>:\\\n",
    "Masked Language Models (MLMs) learn to understand the relationship between words. Additionally, BERT is also trained on the task of Next Sentence Prediction for tasks that require an understanding of the relationship between sentences. In order to understand relationship between two sentences, BERT training process also uses next sentence prediction. A pre-trained model with this kind of understanding is relevant for tasks like question answering. During training the model gets as input pairs of sentences and it learns to predict if the second sentence is the next sentence in the original text as well.\n",
    "\n",
    "As we have seen earlier, BERT separates sentences with a special [SEP] token. During training the model is fed with two input sentences at a time such that:\n",
    "1. 50% of the time the second sentence comes after the first one.\n",
    "2. 50% of the time it is a a random sentence from the full corpus.\n",
    "\n",
    "BERT is then required to predict whether the second sentence is random or not, with the assumption that the random sentence will be disconnected from the first sentence. To predict if the second sentence is connected to the first one or not, basically the complete input sequence goes through the Transformer based model, the output of the [CLS] token is transformed into a 2×1 shaped vector using a simple classification layer, and the IsNext-Label is assigned using softmax. The model is trained with both Masked LM and Next Sentence Prediction together. This is to minimize the combined loss function of the two strategies — “together is better”.\n",
    "\n",
    "<b>Applications:</b>\n",
    "1. Natural Language Inference\n",
    "2. Sentiment Analysis\n",
    "3. Question Answering\n",
    "4. Paraphrase Detection\n",
    "5. Linguistic Acceptability\n",
    "\n",
    "Let's try Text Classification using BERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-pretrained-bert\n",
      "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pytorch-nlp\n",
      "  Downloading pytorch_nlp-0.5.0-py3-none-any.whl (90 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.1/90.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting boto3\n",
      "  Downloading boto3-1.26.96-py3-none-any.whl (135 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.5/135.5 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pytorch-pretrained-bert) (1.23.1)\n",
      "Requirement already satisfied: regex in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pytorch-pretrained-bert) (2022.9.13)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pytorch-pretrained-bert) (4.64.1)\n",
      "Requirement already satisfied: torch>=0.4.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pytorch-pretrained-bert) (1.13.1)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pytorch-pretrained-bert) (2.28.1)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (4.3.0)\n",
      "Collecting s3transfer<0.7.0,>=0.6.0\n",
      "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting botocore<1.30.0,>=1.29.96\n",
      "  Downloading botocore-1.29.96-py3-none-any.whl (10.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests->pytorch-pretrained-bert) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests->pytorch-pretrained-bert) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests->pytorch-pretrained-bert) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests->pytorch-pretrained-bert) (3.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from botocore<1.30.0,>=1.29.96->boto3->pytorch-pretrained-bert) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.96->boto3->pytorch-pretrained-bert) (1.16.0)\n",
      "Installing collected packages: pytorch-nlp, jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
      "Successfully installed boto3-1.26.96 botocore-1.29.96 jmespath-1.0.1 pytorch-nlp-0.5.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.6.0\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install pytorch-pretrained-bert pytorch-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Dataset:</b>\n",
    "\n",
    "The Corpus of Linguistic Acceptability (CoLA) dataset for single sentence classification.\n",
    "It's a set of sentences labeled as grammatically correct or incorrect. The data is as follows:\n",
    ">Column 1: the code representing the source of the sentence. \\\n",
    "Column 2: the acceptability judgment label (0=unacceptable, 1=acceptable). \\\n",
    "Column 3: the acceptability judgment as originally notated by the author. \\\n",
    "Column 4: the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8551, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_source</th>\n",
       "      <th>label</th>\n",
       "      <th>label_notes</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2038</th>\n",
       "      <td>rhl07</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I sent the package all the way around the world.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>r-67</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>That anybody ever left at all is impossible.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6621</th>\n",
       "      <td>m_02</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The truck spread salts.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7494</th>\n",
       "      <td>sks13</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mary sent Bill a book,….</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5761</th>\n",
       "      <td>c_13</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I didn't read a single book the whole time I was in the library.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentence_source  label label_notes  \\\n",
       "2038           rhl07      1         NaN   \n",
       "1992            r-67      1         NaN   \n",
       "6621            m_02      1         NaN   \n",
       "7494           sks13      1         NaN   \n",
       "5761            c_13      1         NaN   \n",
       "\n",
       "                                                              sentence  \n",
       "2038                  I sent the package all the way around the world.  \n",
       "1992                      That anybody ever left at all is impossible.  \n",
       "6621                                           The truck spread salts.  \n",
       "7494                                          Mary sent Bill a book,….  \n",
       "5761  I didn't read a single book the whole time I was in the library.  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sentences and label lists \n",
    "sentences = df.sentence.values\n",
    "\n",
    "# adding special tokens at the begining and end of each sentences for BERT to work properly\n",
    "sentences = [\"[CLS] \" + sentence + \" [SEP] \" for sentence in sentences]\n",
    "labels = df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sentence tokenized:  ['[CLS]', 'our', 'friends', 'won', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# importing the BERT tokenizer, used to convert our text into tokens that corresponds to BERTs vocabulary\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "print(\"First sentence tokenized: \", tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT requires specifically formatted inputs, For each tokenized input sentence, we need to create: \n",
    "1. input ids: a sequence of integers identifying each input token to its index number in the BERT tokenizer vocabulary\n",
    "2. segment mask: (optional) a sequence of 1s and 0s used to identify whether the input is one sentence or two sentences long. For one sentence inputs, this is simply a sequence of 0s. For two sentence inputs, there is a 0 for each token of the first sentence, followed by a 1 for each token of the second sentence\n",
    "3. attention mask: (optional) a sequence of 1s and 0s, with 1s for all input tokens and 0s for all padding tokens (we'll detail this in the next paragraph)\n",
    "4. labels: a single value of 1 or 0. In our task 1 means \"grammatical\" and 0 means \"ungrammatical\"\n",
    "\n",
    "Although we can have variable length input sentences, BERT does requires our input arrays to be the same size. So we can choose a max sentence length and pad or truncate the input as required. \n",
    "\n",
    "pad_sequences is a utility function that we're borrowing from Keras. It simply handles the truncating and padding of Python lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the max length, in original paper it is tkaen as 512\n",
    "max_len = 128\n",
    "\n",
    "# using bert tokenizer to convert the tokens to their index numbers in the bert vocab\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "\n",
    "# padding the input token\n",
    "input_ids = pad_sequences(input_ids, maxlen = max_len, dtype = \"long\", truncating = 'post', padding = 'post')\n",
    "\n",
    "# create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# create a mask of 1s for each token followed by 0s for padding \n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Splitting data into train, validation sets for training:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(\n",
    "    input_ids, labels, random_state = 2018, test_size = 0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(\n",
    "    attention_masks, input_ids, random_state = 2018, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Converting all data into torch tensors:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting batch size, authors recommend 16 or 32 for fine-tuning bert for specific task\n",
    "batch_size = 32\n",
    "\n",
    "# creating an iterator of our data with torch dataloader, helps save on memory during training\n",
    "# unlike a for loop, with an iterator the entire dataset doesn't need to be loaded into the memory\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size = batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler = validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><b>Model Training:</b></u>\n",
    "\n",
    "For this task, we first want to modify the pre-trained BERT model to give outputs for classification, and then we want to continue training the model on our dataset until that the entire model, end-to-end, is well-suited for our task. Thankfully, the huggingface pytorch implementation includes a set of interfaces designed for a variety of NLP tasks. Though these interfaces are all built on top of a trained BERT model, each has different top layers and output types designed to accomodate their specific NLP task. We'll load BertForSequenceClassification. This is the normal BERT model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task.\n",
    "\n",
    "<b>Fine-Tuning Structure & Process:</b> \\\n",
    "The first token of every sequence is the special classification token ([CLS]). Unlike the hidden state vector corresponding to a normal word token, the hidden state corresponding to this special token is designated by the authors of BERT as an aggregate representation of the whole sentence used for classification tasks. As such, when we feed in an input sentence to our model during training, the output is the length 768 hidden state vector corresponding to this token. The additional layer that we've added on top consists of untrained linear neurons of size [hidden_state, number_of_labels], so [768,2], meaning that the output of BERT plus our classification layer is a vector of two numbers representing the \"score\" for \"grammatical/non-grammatical\" that are then fed into cross-entropy loss.\n",
    "\n",
    "Because the pre-trained BERT layers already encode a lot of information about the language, training the classifier is relatively inexpensive. Rather than training every layer in a large model from scratch, it's as if we have already trained the bottom layers 95% of where they need to be, and only really need to train the top layer, with a bit of tweaking going on in the lower levels to accomodate our task. Sometimes practicioners will opt to \"freeze\" certain layers when fine-tuning, or to apply different learning rates, apply diminishing learning rates, etc. all in an effort to preserve the good quality weights in the network and speed up training (often considerably). In fact, recent research on BERT specifically has demonstrated that freezing the majority of the weights results in only minimal accuracy declines, but there are exceptions and broader rules of transfer learning that should also be considered. For example, if your task and fine-tuning dataset is very different from the dataset used to train the transfer learning model, freezing the weights may not be a good idea. We'll cover the broader scope of transfer learning in NLP in a future post.\n",
    "\n",
    "Now, let's load BERT. There are a few different pre-trained BERT models available. \"bert-base-uncased\" means the version that has only lowercase letters (\"uncased\") and is the smaller version of the two (\"base\" vs \"large\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████| 407873900/407873900 [00:38<00:00, 10579582.38B/s]\n"
     ]
    }
   ],
   "source": [
    "# Loading BertForSequenceClassification\n",
    "# pretrained BERT model with a single linear classification layer on top\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our model loaded we need to grab the training hyperparameters from within the stored model. For the purposes of fine-tuning, the authors recommend the following hyperparameter ranges:\n",
    "\n",
    "Batch size: 16, 32 \\\n",
    "Learning rate (Adam): 5e-5, 3e-5, 2e-5 \\\n",
    "Number of epochs: 2, 3, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "                     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "                     'weight_decay_rate': 0.0}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_total value of -1 results in schedule not being applied\n"
     ]
    }
   ],
   "source": [
    "# This variable contains all of the hyperparemeter information our training loop needs\n",
    "optimizer = BertAdam(optimizer_grouped_parameters,lr=2e-5,warmup=.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each pass in the training loop we have a training phase and a validation phase.\n",
    "\n",
    "At each pass we need to:\n",
    "\n",
    "<b>Training loop:</b>\n",
    "<li>Tell the model to compute gradients by setting the model in train mode\n",
    "<li>Unpack our data inputs and labels\n",
    "<li>Load data onto the GPU for acceleration\n",
    "<li>Clear out the gradients calculated in the previous pass. In pytorch the gradients accumulate by default (useful for things like RNNs) unless you explicitly clear them out\n",
    "<li>Forward pass (feed input data through the network)\n",
    "<li>Backward pass (backpropagation)\n",
    "<li>Tell the network to update parameters with optimizer.step()\n",
    "<li>Track variables for monitoring progress</li>\n",
    "    \n",
    "    \n",
    "<b>Evaluation loop:</b>\n",
    "<li>Tell the model not to compute gradients by setting th emodel in evaluation mode\n",
    "<li>Unpack our data inputs and labels\n",
    "<li>Load data onto the GPU for acceleration\n",
    "<li>Forward pass (feed input data through the network)\n",
    "<li>Compute loss on our validation data and track variables for monitoring progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis = 1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = []\n",
    "\n",
    "# storing loss, accuracy for plotting\n",
    "train_loss_set = []\n",
    "\n",
    "# number of training epochs\n",
    "epochs = 2\n",
    "\n",
    "# trange --> tqdm wrapper around basic python range\n",
    "for _ in trange(epochs, desc = \"Epoch\"):  \n",
    "    \n",
    "    ############    \n",
    "    # training #\n",
    "    ############\n",
    "    \n",
    "    # set our model to training mode \n",
    "    model.train()\n",
    "    \n",
    "    # tracking variables\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    \n",
    "    # train data for one epoch\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # add batch to GPU\n",
    "        # batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # unpack inputs from dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # clear out gradients, they accumulate by default\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        loss = model(b_input_ids, token_type_ids = None, attention_mask = b_input_mask, labels = b_labels)\n",
    "        train_loss_set.append(loss.item())\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # update parameters and take a step using the computed gradient\n",
    "        optimizer.step()\n",
    "        \n",
    "        # update tracking variables\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "    \n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    \n",
    "    ##############    \n",
    "    # validation #\n",
    "    ##############\n",
    "    \n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # tracking variables \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    \n",
    "    # evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        # add batch to GPU\n",
    "        # batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # telling model NOT to compute or store gradients, saving memory and speeding up validation\n",
    "        with torch.no_grad():\n",
    "            # forward pass, calculate logit predictions\n",
    "            logits = model(b_input_ids, token_type_ids = None, \n",
    "                               attention_mask = b_input_mask)\n",
    "            \n",
    "        # move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "            \n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "            \n",
    "        eval_accuracy+=tmp_eval_accuracy\n",
    "        nb_eval_steps+=1\n",
    "    \n",
    "    print(\"Validation accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Training Evaluation:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loss over all batches\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(train_loss_set)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Prediction & Evaluation on Holdout Set:</b>\\\n",
    "Loading the holdout dataset and preparing inputs as done earlier. Evaluate predictions using Matthew's Correlation Coefficient (metric used by wider NLP community to evaluate performance on CoLA). +1 is best, -1 is worst. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"cola_public/raw/out_of_domain_dev.tsv\", delimiter='\\t', header=None, \n",
    "                         names=['sentence_source', 'label', 'label_notes', 'sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sentence and label list\n",
    "sentences = df.sentences.values\n",
    "\n",
    "# add special tokens at first & last of each sentence for BERT to work properly\n",
    "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
    "labels = df.label.values\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "max_len = 128\n",
    "\n",
    "# using BERT tokenizer to convert token to their index numbers in the BERT vocab\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "\n",
    "# padding the input\n",
    "input_ids = pad_sequences(input_ids, maxlen = max_len, dtype = \"long\", \n",
    "                                 truncating = \"post\", padding = \"post\")\n",
    "\n",
    "# create attention mask\n",
    "attention_masks = []\n",
    "\n",
    "# create a mask of 1s for each token followed by 0s for padding \n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.apend(seq_mask)\n",
    "    \n",
    "prediction_inputs = torch.tensor(input_ids)\n",
    "prediction_masks = torch.tensor(attention_masks)\n",
    "prediction_labels = torch.tensor(labels)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler = prediction_sampler, batch_size = batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction on test set\n",
    "\n",
    "# model to evaluation model\n",
    "model.eval()\n",
    "\n",
    "# tracking variables \n",
    "prediction, true_labels = [], []\n",
    "\n",
    "# predict \n",
    "for batch in prediction_dataloader:\n",
    "    # add batch to GPU \n",
    "    # batch = tuplet(t.to(device) for t in batch)\n",
    "    # unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    \n",
    "    # telling the model not to compute or store gradients, saving memory and speeding up predictions\n",
    "    with torch.no_grad():\n",
    "        # forward pass, calculate logit predictions\n",
    "        logits = model(b_input_ids, token_type_ids = None, attention_mask = b_input_mask)\n",
    "    \n",
    "    # move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    \n",
    "    # store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Import and evaluate each test batch using Matthew's correlation coefficient:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "matthews_set = []\n",
    "\n",
    "for i in range(len(true_labels)):\n",
    "    matthews = matthews_corrcoef(true_labels[i],\n",
    "                 np.argmax(predictions[i], axis=1).flatten())\n",
    "    matthews_set.append(matthews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other NLP tools continued in next notebook. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
